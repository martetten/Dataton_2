{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–º–ø–æ—Ä—Ç –±–∏–±–∏–ª–∏–æ—Ç–µ–∫   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/raw/1.csv')\n",
    "df2 = pd.read_csv('../data/raw/2.csv')\n",
    "df3 = pd.read_csv('../data/raw/3.csv')\n",
    "df4 = pd.read_csv('../data/raw/4.csv')\n",
    "df5 = pd.read_csv('../data/raw/5.csv')\n",
    "df6 = pd.read_csv('../data/raw/6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   568 non-null    object\n",
      " 2   speech2text  338 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   418 non-null    object\n",
      " 2   speech2text  191 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   521 non-null    object\n",
      " 2   speech2text  202 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   678 non-null    object\n",
      " 2   speech2text  342 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   622 non-null    object\n",
      " 2   speech2text  202 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   613 non-null    object\n",
      " 2   speech2text  596 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df1.info())\n",
    "print(df2.info())\n",
    "print(df3.info())\n",
    "print(df4.info())\n",
    "print(df5.info())\n",
    "print(df6.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_text</th>\n",
       "      <th>image2text</th>\n",
       "      <th>speech2text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚≠êÔ∏è  –ö–Ω–æ–ø–∫–∞: ‚≠êÔ∏èSTART‚≠êÔ∏è(https://t.me/major/start...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ê –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ü—Ä–∞–≤–∏–ª—å–Ω–æ. –í –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...</td>\n",
       "      <td>–¥–µ–≤—á–æ–Ω–∫–∏ –Ω–µ —É–º–µ—é—Ç –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å—Å—è sanille –æ–Ω–∏ —ç...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–¢–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...</td>\n",
       "      <td>10:42 nuil –ø—É–ª telegram ^ 51 142 –ø–æ–¥–ø–∏—Å—á–∏–∫–∞ 12...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–£ –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...</td>\n",
       "      <td>—Ç —Å –Ω–µ ^ –µ z 8 * \\\"8 –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∞ –æ —Å—Ç–µ–Ω–¥–∞–ø...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>–û—Ç–∫–∞–∑–∞–ª–∏ –Ω–æ–≥–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è!  #—â–µ—Ä–±–∞–∫–æ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–ù–æ—á—å, –æ–∂–∏–¥–∞–Ω–∏—è, —Ö–æ–ª–æ–¥, –±–æ–ª—å, —Å–ª–æ–≤–Ω–æ —è —Ä–∞—Å–∫–æ–ª, ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>–û—Å—Ç–æ—Ä–æ–∂–Ω–æ –ø—Ä–æ–ø–∞–ª–∞ —Å–æ–±–∞–∫–∞! #–∞–ª–µ–∫—Å–µ–π—â–µ—Ä–±–∞–∫–æ–≤ #—â–µ...</td>\n",
       "      <td>(–ø–∞–ø—É—Å–æ–≤^ —Ö</td>\n",
       "      <td>–í—Å–µ–≥–¥–∞ –æ—á–µ–Ω—å –æ–±–∏–¥–Ω–æ —á–∏—Ç–∞—Ç—å –ø–æ–¥–æ–±–Ω—ã–µ —Ä–æ–¥–Ω—ã–µ –æ–±—ä...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>–î–ª—è –≤–∞—à–∏—Ö –æ–≥–æ–Ω—ë—á–∫–æ–≤üî•</td>\n",
       "      <td>v –∫ –≤–∏–¥–µ–æ</td>\n",
       "      <td>–Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞—Ö –æ–≥–æ–Ω—ë—á–∫–∏ –≤–¥—Ä—É–≥ –∑–∞–∂–∏–≥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∑–∞...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>#–∑–∏–º–∏–Ω–∞—Ä—É–ª–∏—Ç üòÇüòÇüòÇ #–∞–∫—Ç—Ä–∏—Å–∞—Ç–µ–∞—Ç—Ä–∞–∏–∫–∏–Ω–æ  #–≤–∏–∫—Ç–æ—Ä–∏...</td>\n",
       "      <td>–æ 6</td>\n",
       "      <td>–ß—Ç–æ –≤—ã —É–º–µ–µ—Ç–µ –¥–µ–ª–∞—Ç—å? –í—Å—ë. –°—Ç—Ä–µ–ª—è—Ç—å, –≤–∞—Ä–∏—Ç—å —Ö–∞...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>–î–µ–≤–æ—á–∫–∏, –Ω—É –º—ã?!  @katiashmatia –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—Å—Ç...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–º–∏–ª–∞—è –æ–Ω –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –º–Ω–µ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ —á–µ—Ç—ã...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               doc_text  \\\n",
       "0     –¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...   \n",
       "1     ‚≠êÔ∏è  –ö–Ω–æ–ø–∫–∞: ‚≠êÔ∏èSTART‚≠êÔ∏è(https://t.me/major/start...   \n",
       "2     –ê –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ü—Ä–∞–≤–∏–ª—å–Ω–æ. –í –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...   \n",
       "3     –¢–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...   \n",
       "4     –£ –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...   \n",
       "...                                                 ...   \n",
       "5995  –û—Ç–∫–∞–∑–∞–ª–∏ –Ω–æ–≥–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è!  #—â–µ—Ä–±–∞–∫–æ...   \n",
       "5996  –û—Å—Ç–æ—Ä–æ–∂–Ω–æ –ø—Ä–æ–ø–∞–ª–∞ —Å–æ–±–∞–∫–∞! #–∞–ª–µ–∫—Å–µ–π—â–µ—Ä–±–∞–∫–æ–≤ #—â–µ...   \n",
       "5997                               –î–ª—è –≤–∞—à–∏—Ö –æ–≥–æ–Ω—ë—á–∫–æ–≤üî•   \n",
       "5998  #–∑–∏–º–∏–Ω–∞—Ä—É–ª–∏—Ç üòÇüòÇüòÇ #–∞–∫—Ç—Ä–∏—Å–∞—Ç–µ–∞—Ç—Ä–∞–∏–∫–∏–Ω–æ  #–≤–∏–∫—Ç–æ—Ä–∏...   \n",
       "5999  –î–µ–≤–æ—á–∫–∏, –Ω—É –º—ã?!  @katiashmatia –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—Å—Ç...   \n",
       "\n",
       "                                             image2text  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2     –¥–µ–≤—á–æ–Ω–∫–∏ –Ω–µ —É–º–µ—é—Ç –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å—Å—è sanille –æ–Ω–∏ —ç...   \n",
       "3     10:42 nuil –ø—É–ª telegram ^ 51 142 –ø–æ–¥–ø–∏—Å—á–∏–∫–∞ 12...   \n",
       "4     —Ç —Å –Ω–µ ^ –µ z 8 * \\\"8 –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∞ –æ —Å—Ç–µ–Ω–¥–∞–ø...   \n",
       "...                                                 ...   \n",
       "5995                                                NaN   \n",
       "5996                                        (–ø–∞–ø—É—Å–æ–≤^ —Ö   \n",
       "5997                                          v –∫ –≤–∏–¥–µ–æ   \n",
       "5998                                                –æ 6   \n",
       "5999                                                NaN   \n",
       "\n",
       "                                            speech2text  class  \n",
       "0                                                   NaN      1  \n",
       "1                                                   NaN      1  \n",
       "2                                                   NaN      1  \n",
       "3                                                   NaN      1  \n",
       "4                                                   NaN      1  \n",
       "...                                                 ...    ...  \n",
       "5995  –ù–æ—á—å, –æ–∂–∏–¥–∞–Ω–∏—è, —Ö–æ–ª–æ–¥, –±–æ–ª—å, —Å–ª–æ–≤–Ω–æ —è —Ä–∞—Å–∫–æ–ª, ...      6  \n",
       "5996  –í—Å–µ–≥–¥–∞ –æ—á–µ–Ω—å –æ–±–∏–¥–Ω–æ —á–∏—Ç–∞—Ç—å –ø–æ–¥–æ–±–Ω—ã–µ —Ä–æ–¥–Ω—ã–µ –æ–±—ä...      6  \n",
       "5997  –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞—Ö –æ–≥–æ–Ω—ë—á–∫–∏ –≤–¥—Ä—É–≥ –∑–∞–∂–∏–≥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∑–∞...      6  \n",
       "5998  –ß—Ç–æ –≤—ã —É–º–µ–µ—Ç–µ –¥–µ–ª–∞—Ç—å? –í—Å—ë. –°—Ç—Ä–µ–ª—è—Ç—å, –≤–∞—Ä–∏—Ç—å —Ö–∞...      6  \n",
       "5999  –º–∏–ª–∞—è –æ–Ω –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –º–Ω–µ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ —á–µ—Ç—ã...      6  \n",
       "\n",
       "[6000 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['class'] = 1\n",
    "df2['class'] = 2\n",
    "df3['class'] = 3\n",
    "df4['class'] = 4\n",
    "df5['class'] = 5\n",
    "df6['class'] = 6\n",
    "df_gen = pd.concat([df1, df2, df3, df4, df5, df6]).reset_index(drop=True)\n",
    "df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  class\n",
      "0     –¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...      1\n",
      "1     ‚≠êÔ∏è  –ö–Ω–æ–ø–∫–∞: ‚≠êÔ∏èSTART‚≠êÔ∏è(https://t.me/major/start...      1\n",
      "2     –ê –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ü—Ä–∞–≤–∏–ª—å–Ω–æ. –í –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...      1\n",
      "3     –¢–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...      1\n",
      "4     –£ –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...      1\n",
      "...                                                 ...    ...\n",
      "5995  –û—Ç–∫–∞–∑–∞–ª–∏ –Ω–æ–≥–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è!  #—â–µ—Ä–±–∞–∫–æ...      6\n",
      "5996  –û—Å—Ç–æ—Ä–æ–∂–Ω–æ –ø—Ä–æ–ø–∞–ª–∞ —Å–æ–±–∞–∫–∞! #–∞–ª–µ–∫—Å–µ–π—â–µ—Ä–±–∞–∫–æ–≤ #—â–µ...      6\n",
      "5997  –î–ª—è –≤–∞—à–∏—Ö –æ–≥–æ–Ω—ë—á–∫–æ–≤üî• | v –∫ –≤–∏–¥–µ–æ | –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞...      6\n",
      "5998  #–∑–∏–º–∏–Ω–∞—Ä—É–ª–∏—Ç üòÇüòÇüòÇ #–∞–∫—Ç—Ä–∏—Å–∞—Ç–µ–∞—Ç—Ä–∞–∏–∫–∏–Ω–æ  #–≤–∏–∫—Ç–æ—Ä–∏...      6\n",
      "5999  –î–µ–≤–æ—á–∫–∏, –Ω—É –º—ã?!  @katiashmatia –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—Å—Ç...      6\n",
      "\n",
      "[6000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df_gen.replace('NaN', np.nan)\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ —Ç—Ä–∏ –∫–æ–ª–æ–Ω–∫–∏\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ —Å —É—á–µ—Ç–æ–º NaN –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "def combine_columns(row):\n",
    "    values = [row['doc_text'], row['image2text'], row['speech2text']]\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏ NaN –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    values = [str(v) for v in values if v is not None and not (isinstance(v, float) and np.isnan(v))]\n",
    "    return ' | '.join(values) if values else np.nan\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º\n",
    "df['text'] = df.apply(combine_columns, axis=1)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π DataFrame —Ç–æ–ª—å–∫–æ —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π –∏ –∫–ª–∞—Å—Å–æ–º\n",
    "result_df = df[['text', 'class']]\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(result_df)\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ñ–∞–π–ª\n",
    "# result_df.to_csv('–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π_–¥–∞—Ç–∞—Å–µ—Ç.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vladimir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vladimir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import emoji\n",
    "import inflect\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "def clean_text(text):\n",
    "    text = base_clean_text(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def base_clean_text(text: str):\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    temp = inflect.engine()\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = re.sub('http\\S+', '', word)\n",
    "        word = re.sub('[^\\s–∞-—è—ë–Å–ê-–Øa-zA-Z]', '', word)\n",
    "        if word.isdigit():\n",
    "            words.append(temp.number_to_words(word))\n",
    "        else:\n",
    "            if word not in stop_words:\n",
    "                words.append(word)\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "def emojis_words(text):\n",
    "    # –ú–æ–¥—É–ª—å emoji: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∂–∏ –≤ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç—ë–º –∑–∞–º–µ–Ω—ã \":\" –∏\" _\", –∞ —Ç–∞–∫ –∂–µ - –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–µ–ª–∞ –º–µ–∂–¥—É –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏\n",
    "    text = text.replace(\":\", \"\").replace(\"_\", \" \")\n",
    "    return text\n",
    "def stem_russian_text(text):\n",
    "    stemmer = RussianStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        if re.match('[–∞-—è–ê-–Ø]', word):\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            stemmed_words.append(stemmed_word)\n",
    "        else:\n",
    "            stemmed_words.append(word)\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.FloatTensor(self.labels[idx])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º FloatTensor –¥–ª—è –º—É–ª—å—Ç–∏–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class BertClassifier:\n",
    "\n",
    "    def __init__(self, model_path, tokenizer_path, n_classes=44, epochs=7, \n",
    "                 model_save_path='model_save', tokenizer_save_path='tokenizer_path', \n",
    "                 force_cpu=False, batch_size=16, max_len=512):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model_save_path = model_save_path\n",
    "        self.tokenizer_save_path = tokenizer_save_path\n",
    "        self.max_len = max_len\n",
    "        self.epochs = epochs\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "        if force_cpu:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\")\n",
    "        else:\n",
    "            # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ GPU, –Ω–æ –±—É–¥–µ–º –≥–æ—Ç–æ–≤—ã –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ CPU\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                    # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ GPU\n",
    "                    self.model = BertForSequenceClassification.from_pretrained(\n",
    "                        model_path, \n",
    "                        num_labels=n_classes,\n",
    "                        problem_type=\"multi_label_classification\"\n",
    "                    ).to(self.device)\n",
    "                    # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏, –≤—ã–¥–µ–ª–∏–≤ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä\n",
    "                    test_tensor = torch.zeros((batch_size, max_len), device=self.device)\n",
    "                    del test_tensor  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "                    print(\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU\")\n",
    "                else:\n",
    "                    self.device = torch.device(\"cpu\")\n",
    "                    print(\"GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\")\n",
    "                    self.model = BertForSequenceClassification.from_pretrained(\n",
    "                        model_path, \n",
    "                        num_labels=n_classes,\n",
    "                        problem_type=\"multi_label_classification\"\n",
    "                    ).to(self.device)\n",
    "            except (RuntimeError, torch.cuda.OutOfMemoryError):\n",
    "                # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU\n",
    "                print(\"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU, –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU\")\n",
    "                torch.cuda.empty_cache()  # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU\n",
    "                self.device = torch.device(\"cpu\")\n",
    "                # –ó–∞–Ω–æ–≤–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU\n",
    "                self.model = BertForSequenceClassification.from_pretrained(\n",
    "                    model_path, \n",
    "                    num_labels=n_classes,\n",
    "                    problem_type=\"multi_label_classification\"\n",
    "                ).to(self.device)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.model_path, num_labels=self.n_classes)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def tokenize_texts(self, texts, tokenizer, max_length):\n",
    "        tokenized_texts = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        return tokenized_texts\n",
    "\n",
    "    def preparation(self, X_train, y_train, X_valid, y_valid):\n",
    "        # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã, –∫–∞–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö\n",
    "        train_encodings = self.tokenize_texts(X_train, self.tokenizer, self.max_len)\n",
    "        valid_encodings = self.tokenize_texts(X_valid, self.tokenizer, self.max_len)\n",
    "        \n",
    "        # create datasets\n",
    "        self.train_set = CustomDataset(train_encodings, y_train)\n",
    "        self.valid_set = CustomDataset(valid_encodings, y_valid)\n",
    "\n",
    "        # create data loaders —Å –º–µ–Ω—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ –Ω–∞ CPU\n",
    "        if self.device.type == 'cpu':\n",
    "            # –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è CPU\n",
    "            cpu_batch_size = max(1, self.batch_size // 2)\n",
    "            print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ CPU: {cpu_batch_size}\")\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=cpu_batch_size)\n",
    "            self.valid_loader = DataLoader(self.valid_set, batch_size=cpu_batch_size)\n",
    "        else:\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size)\n",
    "            self.valid_loader = DataLoader(self.valid_set, batch_size=self.batch_size)\n",
    "\n",
    "        # helpers initialization —Å –º–µ–Ω—å—à–∏–º learning rate –Ω–∞ CPU\n",
    "        if self.device.type == 'cpu':\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=1e-5)  # –ú–µ–Ω—å—à–∏–π lr –¥–ª—è CPU\n",
    "        else:\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=2e-5)\n",
    "\n",
    "    def fit(self):\n",
    "        self.model.train()\n",
    "        losses = 0\n",
    "        \n",
    "        for data in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            input_ids = data[\"input_ids\"].to(self.device)\n",
    "            attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "            labels = data[\"labels\"].to(self.device)\n",
    "            \n",
    "            # –î–ª—è –º—É–ª—å—Ç–∏–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ labels –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å float –∏ –∏–º–µ—Ç—å —Ñ–æ—Ä–º—É [batch_size, n_classes]\n",
    "            # –ï—Å–ª–∏ –≤–∞—à–∏ –º–µ—Ç–∫–∏ –Ω–µ –≤ —Ç–∞–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –∏—Ö –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å\n",
    "            \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            losses += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        train_loss = losses / len(self.train_loader)\n",
    "        return train_loss\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        threshold = 0.5\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in self.valid_loader:\n",
    "                input_ids = data[\"input_ids\"].to(self.device)\n",
    "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "                labels = data[\"labels\"].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= threshold).float().cpu().numpy()\n",
    "                \n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "                \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º F1 (micro, macro, weighted –∏–ª–∏ samples)\n",
    "        f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        return f1_micro, f1_macro\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
    "            train_acc, train_loss = self.fit()\n",
    "            print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "        self.model.save_pretrained(self.model_save_path)\n",
    "        self.tokenizer.save_pretrained(self.tokenizer_save_path)\n",
    "\n",
    "\n",
    "def predict(self, texts: list, threshold=0.5):\n",
    "    \"\"\"\n",
    "    –ú–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–µ—Ç–æ–∫ –≤ –∑–∞–¥–∞—á–µ –º—É–ª—å—Ç–∏–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        threshold (float): –ü–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è –º–µ—Ç–∫–∏ (0.0-1.0)\n",
    "        \n",
    "    Returns:\n",
    "        list: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∏–Ω–¥–µ–∫—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "              –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –Ω–∏ –∫ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Å—É\n",
    "    \"\"\"\n",
    "    self.model.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏\n",
    "    batch_size = 16\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            encoding = self.tokenizer(\n",
    "                batch_texts,\n",
    "                max_length=self.max_len,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors='pt',\n",
    "            ).to(self.device)\n",
    "            \n",
    "            outputs = self.model(**encoding)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º sigmoid –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            \n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "            batch_preds = (probabilities >= threshold).cpu().numpy()\n",
    "            all_predictions.extend(batch_preds)\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "    result = []\n",
    "    for pred in all_predictions:\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤, –≥–¥–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ—Ä–æ–≥\n",
    "        class_indices = np.where(pred)[0].tolist()\n",
    "        result.append(class_indices)  # –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º, –µ—Å–ª–∏ –Ω–µ—Ç –∫–ª–∞—Å—Å–æ–≤\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: (6000, 2)\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: 6\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
      "class\n",
      "1    1000\n",
      "2    1000\n",
      "3    1000\n",
      "4    1000\n",
      "5    1000\n",
      "6    1000\n",
      "Name: count, dtype: int64\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏: 5620\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: 6\n",
      "–§–æ—Ä–º–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: (4496, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "df = result_df\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(f\"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: {df['class'].nunique()}\")\n",
    "print(f\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\\n{df['class'].value_counts()}\")\n",
    "\n",
    "\n",
    "grouped_texts = df.groupby('text')['class'].apply(list).reset_index()\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏: {len(grouped_texts)}\")\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    grouped_texts['text'], \n",
    "    grouped_texts['class'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_binary = mlb.fit_transform(y_train)\n",
    "y_test_binary = mlb.transform(y_test)\n",
    "\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {len(mlb.classes_)}\")\n",
    "print(f\"–§–æ—Ä–º–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {y_train_binary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU, –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ CPU: 8\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94275/4009181969.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     12\u001b[39m classifier.preparation(\n\u001b[32m     13\u001b[39m     X_train.tolist(),\n\u001b[32m     14\u001b[39m     y_train_binary.tolist(),  \u001b[38;5;66;03m# –í–∞–∂–Ω–æ: –ø–µ—Ä–µ–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏\u001b[39;00m\n\u001b[32m     15\u001b[39m     X_test.tolist(),\n\u001b[32m     16\u001b[39m     y_test_binary.tolist()    \u001b[38;5;66;03m# –í–∞–∂–Ω–æ: –ø–µ—Ä–µ–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\u001b[39;00m\n\u001b[32m     23\u001b[39m texts_to_predict = [\u001b[33m\"\u001b[39m\u001b[33m–†–µ—à–∏–ª–∏ –ø—Ä–æ–≥—É–ª—è—Ç—å—Å—è —Å —Å–µ–º—å–µ–π –ø–æ —Ç–æ—Ä–≥–æ–≤–æ–º—É —Ü–µ–Ω—Ç—Ä—É. –û–±–æ–∂–∞—é —Ç–∞–∫–∏–µ —Å–µ–º–µ–π–Ω—ã–µ –ø–æ—Å–∏–¥–µ–ª–∫–∏ –∏ –≤—Å–µ —Ç–∞–∫–æ–µ. –ö–æ–≥–¥–∞ –ø—Ä–æ–≥–æ–ª–æ–¥–∞–ª–∏—Å—å, –≤—Å–ø–æ–º–Ω–∏–ª–∏ –ø—Ä–æ –í–∫—É—Å—Å-–í–∏–ª–ª–∞. –í –Ω–µ–º –≤—Å–µ–≥–¥–∞ –≤—Å–µ —Å–≤–µ–∂–µ–µ –∏ –ø–æ–ª–µ–∑–Ω–æ–µ\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36mBertClassifier.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.epochs):\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     train_acc, train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    179\u001b[39m \u001b[38;5;28mself\u001b[39m.model.save_pretrained(\u001b[38;5;28mself\u001b[39m.model_save_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mBertClassifier.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    131\u001b[39m     loss = outputs.loss\n\u001b[32m    132\u001b[39m     losses += loss.item()\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m    136\u001b[39m train_loss = losses / \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "classifier = BertClassifier(\n",
    "    model_path=\"DeepPavlov/rubert-base-cased\", # –∏–ª–∏ –¥—Ä—É–≥–∞—è –ø–æ–¥—Ö–æ–¥—è—â–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "    tokenizer_path=\"DeepPavlov/rubert-base-cased\",\n",
    "    n_classes=len(mlb.classes_),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤\n",
    "    epochs=3,\n",
    "    model_save_path='rubert_hackothon',\n",
    "    tokenizer_save_path='rubert_hackothon'\n",
    ")\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "classifier.preparation(\n",
    "    X_train.tolist(),\n",
    "    y_train_binary.tolist(), \n",
    "    X_test.tolist(),\n",
    "    y_test_binary.tolist()  \n",
    ")\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "classifier.train()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "texts_to_predict = [\"–†–µ—à–∏–ª–∏ –ø—Ä–æ–≥—É–ª—è—Ç—å—Å—è —Å —Å–µ–º—å–µ–π –ø–æ —Ç–æ—Ä–≥–æ–≤–æ–º—É —Ü–µ–Ω—Ç—Ä—É. –û–±–æ–∂–∞—é —Ç–∞–∫–∏–µ —Å–µ–º–µ–π–Ω—ã–µ –ø–æ—Å–∏–¥–µ–ª–∫–∏ –∏ –≤—Å–µ —Ç–∞–∫–æ–µ. –ö–æ–≥–¥–∞ –ø—Ä–æ–≥–æ–ª–æ–¥–∞–ª–∏—Å—å, –≤—Å–ø–æ–º–Ω–∏–ª–∏ –ø—Ä–æ –í–∫—É—Å—Å-–í–∏–ª–ª–∞. –í –Ω–µ–º –≤—Å–µ–≥–¥–∞ –≤—Å–µ —Å–≤–µ–∂–µ–µ –∏ –ø–æ–ª–µ–∑–Ω–æ–µ\"]\n",
    "predictions = classifier.predict(texts_to_predict)\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤\n",
    "for text, pred_indices in zip(texts_to_predict, predictions):\n",
    "    if pred_indices:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–ª–∞—Å—Å\n",
    "        pred_classes = [mlb.classes_[idx] for idx in pred_indices]\n",
    "        print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã: {pred_classes}\")\n",
    "    else:\n",
    "        print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "        print(\"–¢–µ–∫—Å—Ç –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –Ω–∏ –∫ –æ–¥–Ω–æ–º—É –∏–∑ –∫–ª–∞—Å—Å–æ–≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4377/1028348104.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.0899015083564279 accuracy 0.5634460547504025\n",
      "Epoch 2/5\n",
      "Train loss 0.6159625046702154 accuracy 0.765378421900161\n",
      "Epoch 3/5\n",
      "Train loss 0.358038229357863 accuracy 0.8668276972624799\n",
      "Epoch 4/5\n",
      "Train loss 0.2010907255793062 accuracy 0.9299516908212561\n",
      "Epoch 5/5\n",
      "Train loss 0.13264704557557072 accuracy 0.9553945249597423\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: ['–ª–∏—á–Ω–∞—è –∂–∏–∑–Ω—å', '—Å–æ—Ü—Å–µ—Ç–∏']\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "# # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ `dataset` - —ç—Ç–æ –≤–∞—à –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ \"–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è\" –∏ \"–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\"\n",
    "\n",
    "# # –°–æ–∑–¥–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "# label_mapping = {cat: i for i, cat in enumerate(result_df['class'].unique())}\n",
    "# dataset['label'] = dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].map(label_mapping)\n",
    "\n",
    "# # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     dataset['–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è'].tolist(), \n",
    "#     dataset['label'].tolist(),\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     stratify=dataset['label']  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫ –≤ –≤—ã–±–æ—Ä–∫–∞—Ö\n",
    "# )\n",
    "\n",
    "# # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "# # –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å BERT\n",
    "# model_path = \"DeepPavlov/rubert-base-cased\"  # –∏–ª–∏ –¥—Ä—É–≥—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å\n",
    "# tokenizer_path = \"DeepPavlov/rubert-base-cased\"\n",
    "# n_classes = len(label_mapping)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "\n",
    "# classifier = BertClassifier(\n",
    "#     model_path=model_path,\n",
    "#     tokenizer_path=tokenizer_path,\n",
    "#     n_classes=n_classes,\n",
    "#     epochs=5  # –ú–æ–∂–µ—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
    "# )\n",
    "\n",
    "# # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "# classifier.preparation(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# # 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "# classifier.train()\n",
    "\n",
    "# # 5. –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "# # –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –ø–µ—Ä–≤—ã—Ö 5 —Å–æ–æ–±—â–µ–Ω–∏–π:\n",
    "# sample_texts = dataset['–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è'].head(5).tolist()\n",
    "# predicted_categories = classifier.predict(sample_texts)\n",
    "\n",
    "# # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç–æ–∫ –æ–±—Ä–∞—Ç–Ω–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "# reverse_mapping = {i: cat for cat, i in label_mapping.items()}\n",
    "# predicted_category_names = [reverse_mapping.get(pred-1) for pred in predicted_categories]\n",
    "\n",
    "# print(\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:\", predicted_category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 6. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 4. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\u001b[39;00m\n\u001b[32m     49\u001b[39m accuracy = accuracy_score(y_test, all_predictions)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m report = \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m conf_matrix = confusion_matrix(y_test, all_predictions)\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2693\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2687\u001b[39m         warnings.warn(\n\u001b[32m   2688\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2689\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[32m   2690\u001b[39m             )\n\u001b[32m   2691\u001b[39m         )\n\u001b[32m   2692\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2693\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2694\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m. Try specifying the labels \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparameter\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[32m   2697\u001b[39m         )\n\u001b[32m   2698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2699\u001b[39m     target_names = [\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[31mValueError\u001b[39m: Number of classes, 7, does not match size of target_names, 6. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞\n",
    "# model_path = 'tiny_bert_44cat'\n",
    "# tokenizer_path = 'tiny_bert_44cat_tokenizer'\n",
    "\n",
    "# # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\n",
    "# classifier = BertClassifier(\n",
    "#     model_path=model_path,\n",
    "#     tokenizer_path=tokenizer_path,\n",
    "#     n_classes=6,\n",
    "# )\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n",
    "# classifier.load_model()\n",
    "\n",
    "# # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "# # –ï—Å–ª–∏ —É –≤–∞—Å –µ—â–µ –Ω–µ—Ç —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏, –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –µ–µ –∏–∑ –∏–º–µ—é—â–µ–≥–æ—Å—è –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # –°–æ–∑–¥–∞–¥–∏–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "# categories = dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].unique()\n",
    "# label_mapping = {cat: i for i, cat in enumerate(categories)}\n",
    "# reverse_mapping = {i: cat for i, cat in label_mapping.items()}\n",
    "\n",
    "# # –†–∞–∑–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ (–µ—Å–ª–∏ –µ—â–µ –Ω–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã)\n",
    "# _, X_test, _, y_test = train_test_split(\n",
    "#     dataset['–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è'].tolist(),\n",
    "#     dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].map(label_mapping).tolist(),\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     stratify=dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].map(label_mapping)\n",
    "# )\n",
    "\n",
    "# # 3. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "# # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "# all_predictions = []\n",
    "# for text in X_test:\n",
    "#     # predict –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "#     pred_indices = classifier.predict([text])\n",
    "#     # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é (—Å–∞–º—É—é –≤–µ—Ä–æ—è—Ç–Ω—É—é) –∫–∞—Ç–µ–≥–æ—Ä–∏—é\n",
    "#     all_predictions.append(pred_indices[0] - 1)  # –í—ã—á–∏—Ç–∞–µ–º 1, —Ç.–∫. predict –¥–æ–±–∞–≤–ª—è–µ—Ç 1\n",
    "\n",
    "# # 4. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n",
    "# accuracy = accuracy_score(y_test, all_predictions)\n",
    "# report = classification_report(y_test, all_predictions, target_names=categories)\n",
    "# conf_matrix = confusion_matrix(y_test, all_predictions)\n",
    "\n",
    "# print(f\"–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {accuracy:.4f}\")\n",
    "# print(\"\\n–û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\")\n",
    "# print(report)\n",
    "\n",
    "# print(\"\\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "#             xticklabels=categories, yticklabels=categories)\n",
    "# plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
    "# plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
    "# plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('confusion_matrix.png')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\n",
      "–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ó–¥–æ—Ä–æ–≤—å–µ –∏ –°–ø–æ—Ä—Ç: 5\n",
      "–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Æ–º–æ—Ä –∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è: 4\n",
      "–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ª–∏—á–Ω–∞—è –∂–∏–∑–Ω—å: 0\n",
      "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ä–µ–∫–ª–∞–º–∞: 3\n",
      "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å–æ—Ü—Å–µ—Ç–∏: 1\n",
      "–ö–∞—Ç–µ–≥–æ—Ä–∏—è üåé –ü–æ–ª–∏—Ç–∏–∫–∞: 2\n"
     ]
    }
   ],
   "source": [
    "# original_categories = dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].unique()\n",
    "# label_mapping = {cat: i for i, cat in enumerate(original_categories)}\n",
    "# reverse_mapping = {i: cat for i, cat in label_mapping.items()}\n",
    "\n",
    "# print(\"–ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\")\n",
    "# for i, cat in sorted(reverse_mapping.items()):\n",
    "#     print(f\"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {i}: {cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6754e-04, 1.5310e-03, 9.8409e-01, 7.6318e-04, 9.4161e-04, 1.2308e-02]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier = BertClassifier(\n",
    "#     model_path=model_path,\n",
    "#     tokenizer_path=tokenizer_path,\n",
    "#     n_classes=6,\n",
    "# )\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n",
    "# classifier.load_model()\n",
    "# classifier.predict('–°–µ–≥–æ–¥–Ω—è –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∞ –≤—Å—Ç—Ä–µ—á–∞ –ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—è –≥–æ—Å–¥—É–º—ã –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π —Ä–µ–≥–∏–æ–Ω–æ–≤')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

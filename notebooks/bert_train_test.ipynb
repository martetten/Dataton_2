{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–º–ø–æ—Ä—Ç –±–∏–±–∏–ª–∏–æ—Ç–µ–∫   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "import re\n",
    "import emoji\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(os.path.join('..', 'data', 'raw', '1.csv'))\n",
    "df2 = pd.read_csv(os.path.join('..', 'data', 'raw', '2.csv'))\n",
    "df3 = pd.read_csv(os.path.join('..', 'data', 'raw', '3.csv'))\n",
    "df4 = pd.read_csv(os.path.join('..', 'data', 'raw', '4.csv'))\n",
    "df5 = pd.read_csv(os.path.join('..', 'data', 'raw', '5.csv'))\n",
    "df6 = pd.read_csv(os.path.join('..', 'data', 'raw', '6.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   568 non-null    object\n",
      " 2   speech2text  338 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   418 non-null    object\n",
      " 2   speech2text  191 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   521 non-null    object\n",
      " 2   speech2text  202 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   678 non-null    object\n",
      " 2   speech2text  342 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   622 non-null    object\n",
      " 2   speech2text  202 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   613 non-null    object\n",
      " 2   speech2text  596 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df1.info())\n",
    "print(df2.info())\n",
    "print(df3.info())\n",
    "print(df4.info())\n",
    "print(df5.info())\n",
    "print(df6.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_text</th>\n",
       "      <th>image2text</th>\n",
       "      <th>speech2text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚≠êÔ∏è  –ö–Ω–æ–ø–∫–∞: ‚≠êÔ∏èSTART‚≠êÔ∏è(https://t.me/major/start...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ê –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ü—Ä–∞–≤–∏–ª—å–Ω–æ. –í –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...</td>\n",
       "      <td>–¥–µ–≤—á–æ–Ω–∫–∏ –Ω–µ —É–º–µ—é—Ç –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å—Å—è sanille –æ–Ω–∏ —ç...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–¢–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...</td>\n",
       "      <td>10:42 nuil –ø—É–ª telegram ^ 51 142 –ø–æ–¥–ø–∏—Å—á–∏–∫–∞ 12...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–£ –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...</td>\n",
       "      <td>—Ç —Å –Ω–µ ^ –µ z 8 * \\\"8 –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∞ –æ —Å—Ç–µ–Ω–¥–∞–ø...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>–û—Ç–∫–∞–∑–∞–ª–∏ –Ω–æ–≥–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è!  #—â–µ—Ä–±–∞–∫–æ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–ù–æ—á—å, –æ–∂–∏–¥–∞–Ω–∏—è, —Ö–æ–ª–æ–¥, –±–æ–ª—å, —Å–ª–æ–≤–Ω–æ —è —Ä–∞—Å–∫–æ–ª, ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>–û—Å—Ç–æ—Ä–æ–∂–Ω–æ –ø—Ä–æ–ø–∞–ª–∞ —Å–æ–±–∞–∫–∞! #–∞–ª–µ–∫—Å–µ–π—â–µ—Ä–±–∞–∫–æ–≤ #—â–µ...</td>\n",
       "      <td>(–ø–∞–ø—É—Å–æ–≤^ —Ö</td>\n",
       "      <td>–í—Å–µ–≥–¥–∞ –æ—á–µ–Ω—å –æ–±–∏–¥–Ω–æ —á–∏—Ç–∞—Ç—å –ø–æ–¥–æ–±–Ω—ã–µ —Ä–æ–¥–Ω—ã–µ –æ–±—ä...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>–î–ª—è –≤–∞—à–∏—Ö –æ–≥–æ–Ω—ë—á–∫–æ–≤üî•</td>\n",
       "      <td>v –∫ –≤–∏–¥–µ–æ</td>\n",
       "      <td>–Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞—Ö –æ–≥–æ–Ω—ë—á–∫–∏ –≤–¥—Ä—É–≥ –∑–∞–∂–∏–≥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∑–∞...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>#–∑–∏–º–∏–Ω–∞—Ä—É–ª–∏—Ç üòÇüòÇüòÇ #–∞–∫—Ç—Ä–∏—Å–∞—Ç–µ–∞—Ç—Ä–∞–∏–∫–∏–Ω–æ  #–≤–∏–∫—Ç–æ—Ä–∏...</td>\n",
       "      <td>–æ 6</td>\n",
       "      <td>–ß—Ç–æ –≤—ã —É–º–µ–µ—Ç–µ –¥–µ–ª–∞—Ç—å? –í—Å—ë. –°—Ç—Ä–µ–ª—è—Ç—å, –≤–∞—Ä–∏—Ç—å —Ö–∞...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>–î–µ–≤–æ—á–∫–∏, –Ω—É –º—ã?!  @katiashmatia –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—Å—Ç...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–º–∏–ª–∞—è –æ–Ω –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –º–Ω–µ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ —á–µ—Ç—ã...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               doc_text  \\\n",
       "0     –¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...   \n",
       "1     ‚≠êÔ∏è  –ö–Ω–æ–ø–∫–∞: ‚≠êÔ∏èSTART‚≠êÔ∏è(https://t.me/major/start...   \n",
       "2     –ê –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ü—Ä–∞–≤–∏–ª—å–Ω–æ. –í –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...   \n",
       "3     –¢–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...   \n",
       "4     –£ –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...   \n",
       "...                                                 ...   \n",
       "5995  –û—Ç–∫–∞–∑–∞–ª–∏ –Ω–æ–≥–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è!  #—â–µ—Ä–±–∞–∫–æ...   \n",
       "5996  –û—Å—Ç–æ—Ä–æ–∂–Ω–æ –ø—Ä–æ–ø–∞–ª–∞ —Å–æ–±–∞–∫–∞! #–∞–ª–µ–∫—Å–µ–π—â–µ—Ä–±–∞–∫–æ–≤ #—â–µ...   \n",
       "5997                               –î–ª—è –≤–∞—à–∏—Ö –æ–≥–æ–Ω—ë—á–∫–æ–≤üî•   \n",
       "5998  #–∑–∏–º–∏–Ω–∞—Ä—É–ª–∏—Ç üòÇüòÇüòÇ #–∞–∫—Ç—Ä–∏—Å–∞—Ç–µ–∞—Ç—Ä–∞–∏–∫–∏–Ω–æ  #–≤–∏–∫—Ç–æ—Ä–∏...   \n",
       "5999  –î–µ–≤–æ—á–∫–∏, –Ω—É –º—ã?!  @katiashmatia –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—Å—Ç...   \n",
       "\n",
       "                                             image2text  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2     –¥–µ–≤—á–æ–Ω–∫–∏ –Ω–µ —É–º–µ—é—Ç –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å—Å—è sanille –æ–Ω–∏ —ç...   \n",
       "3     10:42 nuil –ø—É–ª telegram ^ 51 142 –ø–æ–¥–ø–∏—Å—á–∏–∫–∞ 12...   \n",
       "4     —Ç —Å –Ω–µ ^ –µ z 8 * \\\"8 –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∞ –æ —Å—Ç–µ–Ω–¥–∞–ø...   \n",
       "...                                                 ...   \n",
       "5995                                                NaN   \n",
       "5996                                        (–ø–∞–ø—É—Å–æ–≤^ —Ö   \n",
       "5997                                          v –∫ –≤–∏–¥–µ–æ   \n",
       "5998                                                –æ 6   \n",
       "5999                                                NaN   \n",
       "\n",
       "                                            speech2text  class  \n",
       "0                                                   NaN      1  \n",
       "1                                                   NaN      1  \n",
       "2                                                   NaN      1  \n",
       "3                                                   NaN      1  \n",
       "4                                                   NaN      1  \n",
       "...                                                 ...    ...  \n",
       "5995  –ù–æ—á—å, –æ–∂–∏–¥–∞–Ω–∏—è, —Ö–æ–ª–æ–¥, –±–æ–ª—å, —Å–ª–æ–≤–Ω–æ —è —Ä–∞—Å–∫–æ–ª, ...      6  \n",
       "5996  –í—Å–µ–≥–¥–∞ –æ—á–µ–Ω—å –æ–±–∏–¥–Ω–æ —á–∏—Ç–∞—Ç—å –ø–æ–¥–æ–±–Ω—ã–µ —Ä–æ–¥–Ω—ã–µ –æ–±—ä...      6  \n",
       "5997  –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞—Ö –æ–≥–æ–Ω—ë—á–∫–∏ –≤–¥—Ä—É–≥ –∑–∞–∂–∏–≥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∑–∞...      6  \n",
       "5998  –ß—Ç–æ –≤—ã —É–º–µ–µ—Ç–µ –¥–µ–ª–∞—Ç—å? –í—Å—ë. –°—Ç—Ä–µ–ª—è—Ç—å, –≤–∞—Ä–∏—Ç—å —Ö–∞...      6  \n",
       "5999  –º–∏–ª–∞—è –æ–Ω –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –º–Ω–µ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ —á–µ—Ç—ã...      6  \n",
       "\n",
       "[6000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['class'] = 1\n",
    "df2['class'] = 2\n",
    "df3['class'] = 3\n",
    "df4['class'] = 4\n",
    "df5['class'] = 5\n",
    "df6['class'] = 6\n",
    "df_gen = pd.concat([df1, df2, df3, df4, df5, df6]).reset_index(drop=True)\n",
    "df_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ –æ–¥–Ω—É –∫–æ–ª–æ–Ω–∫—É –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  class\n",
      "0     –¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...      1\n",
      "1     ‚≠êÔ∏è  –ö–Ω–æ–ø–∫–∞: ‚≠êÔ∏èSTART‚≠êÔ∏è(https://t.me/major/start...      1\n",
      "2     –ê –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ü—Ä–∞–≤–∏–ª—å–Ω–æ. –í –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...      1\n",
      "3     –¢–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...      1\n",
      "4     –£ –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...      1\n",
      "...                                                 ...    ...\n",
      "5995  –û—Ç–∫–∞–∑–∞–ª–∏ –Ω–æ–≥–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è!  #—â–µ—Ä–±–∞–∫–æ...      6\n",
      "5996  –û—Å—Ç–æ—Ä–æ–∂–Ω–æ –ø—Ä–æ–ø–∞–ª–∞ —Å–æ–±–∞–∫–∞! #–∞–ª–µ–∫—Å–µ–π—â–µ—Ä–±–∞–∫–æ–≤ #—â–µ...      6\n",
      "5997  –î–ª—è –≤–∞—à–∏—Ö –æ–≥–æ–Ω—ë—á–∫–æ–≤üî• | v –∫ –≤–∏–¥–µ–æ | –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞...      6\n",
      "5998  #–∑–∏–º–∏–Ω–∞—Ä—É–ª–∏—Ç üòÇüòÇüòÇ #–∞–∫—Ç—Ä–∏—Å–∞—Ç–µ–∞—Ç—Ä–∞–∏–∫–∏–Ω–æ  #–≤–∏–∫—Ç–æ—Ä–∏...      6\n",
      "5999  –î–µ–≤–æ—á–∫–∏, –Ω—É –º—ã?!  @katiashmatia –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—Å—Ç...      6\n",
      "\n",
      "[6000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df_gen.replace('NaN', np.nan)\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ —Ç—Ä–∏ –∫–æ–ª–æ–Ω–∫–∏\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ —Å —É—á–µ—Ç–æ–º NaN –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "def combine_columns(row):\n",
    "    values = [row['doc_text'], row['image2text'], row['speech2text']]\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏ NaN –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    values = [str(v) for v in values if v is not None and not (isinstance(v, float) and np.isnan(v))]\n",
    "    return ' | '.join(values) if values else np.nan\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º\n",
    "df['text'] = df.apply(combine_columns, axis=1)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π DataFrame —Ç–æ–ª—å–∫–æ —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π –∏ –∫–ª–∞—Å—Å–æ–º\n",
    "result_df = df[['text', 'class']]\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥–≥–æ—Ç–æ–≤–ª–∏–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\avmar\\AppData\\Local\\Temp\\ipykernel_34928\\912105886.py:21: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  word = re.sub('http\\S+', '', word)\n",
      "C:\\Users\\avmar\\AppData\\Local\\Temp\\ipykernel_34928\\912105886.py:22: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  word = re.sub('[^\\s–∞-—è—ë–Å–ê-–Øa-zA-Z]', '', word)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avmar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\avmar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "def clean_text(text):\n",
    "    text = base_clean_text(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def base_clean_text(text: str):\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    temp = inflect.engine()\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = re.sub('http\\S+', '', word)\n",
    "        word = re.sub('[^\\s–∞-—è—ë–Å–ê-–Øa-zA-Z]', '', word)\n",
    "        if word.isdigit():\n",
    "            words.append(temp.number_to_words(word))\n",
    "        else:\n",
    "            if word not in stop_words:\n",
    "                words.append(word)\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "def emojis_words(text):\n",
    "    # –ú–æ–¥—É–ª—å emoji: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∂–∏ –≤ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç—ë–º –∑–∞–º–µ–Ω—ã \":\" –∏\" _\", –∞ —Ç–∞–∫ –∂–µ - –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–µ–ª–∞ –º–µ–∂–¥—É –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏\n",
    "    text = text.replace(\":\", \"\").replace(\"_\", \" \")\n",
    "    return text\n",
    "def stem_russian_text(text):\n",
    "    stemmer = RussianStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        if re.match('[–∞-—è–ê-–Ø]', word):\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            stemmed_words.append(stemmed_word)\n",
    "        else:\n",
    "            stemmed_words.append(word)\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–∏—à–µ–º –µ–¥–∏–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ RuBERT –∏ –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ä–∞–±–æ—Ç—ã —Å –Ω–µ–π. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.FloatTensor(self.labels[idx])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º FloatTensor –¥–ª—è –º—É–ª—å—Ç–∏–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class BertClassifier:\n",
    "\n",
    "    def __init__(self, model_path, tokenizer_path, n_classes=44, epochs=7, \n",
    "                 model_save_path='model_save', tokenizer_save_path='tokenizer_path', \n",
    "                 force_cpu=False, batch_size=16, max_len=512):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model_save_path = model_save_path\n",
    "        self.tokenizer_save_path = tokenizer_save_path\n",
    "        self.max_len = max_len\n",
    "        self.epochs = epochs\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "        if force_cpu:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\")\n",
    "        else:\n",
    "            # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ GPU, –Ω–æ –±—É–¥–µ–º –≥–æ—Ç–æ–≤—ã –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ CPU\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                    # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ GPU\n",
    "                    self.model = BertForSequenceClassification.from_pretrained(\n",
    "                        model_path, \n",
    "                        num_labels=n_classes,\n",
    "                        problem_type=\"multi_label_classification\"\n",
    "                    ).to(self.device)\n",
    "                    # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏, –≤—ã–¥–µ–ª–∏–≤ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä\n",
    "                    test_tensor = torch.zeros((batch_size, max_len), device=self.device)\n",
    "                    del test_tensor  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "                    print(\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU\")\n",
    "                else:\n",
    "                    self.device = torch.device(\"cpu\")\n",
    "                    print(\"GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\")\n",
    "                    self.model = BertForSequenceClassification.from_pretrained(\n",
    "                        model_path, \n",
    "                        num_labels=n_classes,\n",
    "                        problem_type=\"multi_label_classification\"\n",
    "                    ).to(self.device)\n",
    "            except (RuntimeError, torch.cuda.OutOfMemoryError):\n",
    "                # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU\n",
    "                print(\"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU, –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU\")\n",
    "                torch.cuda.empty_cache()  # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU\n",
    "                self.device = torch.device(\"cpu\")\n",
    "                # –ó–∞–Ω–æ–≤–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU\n",
    "                self.model = BertForSequenceClassification.from_pretrained(\n",
    "                    model_path, \n",
    "                    num_labels=n_classes,\n",
    "                    problem_type=\"multi_label_classification\"\n",
    "                ).to(self.device)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.model_path, num_labels=self.n_classes)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def tokenize_texts(self, texts, tokenizer, max_length):\n",
    "        tokenized_texts = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        return tokenized_texts\n",
    "\n",
    "    def preparation(self, X_train, y_train, X_valid, y_valid):\n",
    "        train_encodings = self.tokenize_texts(X_train, self.tokenizer, self.max_len)\n",
    "        valid_encodings = self.tokenize_texts(X_valid, self.tokenizer, self.max_len)\n",
    "        \n",
    "        # create datasets\n",
    "        self.train_set = CustomDataset(train_encodings, y_train)\n",
    "        self.valid_set = CustomDataset(valid_encodings, y_valid)\n",
    "\n",
    "        # create data loaders —Å –º–µ–Ω—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ –Ω–∞ CPU\n",
    "        if self.device.type == 'cpu':\n",
    "            # –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è CPU\n",
    "            cpu_batch_size = max(1, self.batch_size // 2)\n",
    "            print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ CPU: {cpu_batch_size}\")\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=cpu_batch_size)\n",
    "            self.valid_loader = DataLoader(self.valid_set, batch_size=cpu_batch_size)\n",
    "        else:\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size)\n",
    "            self.valid_loader = DataLoader(self.valid_set, batch_size=self.batch_size)\n",
    "\n",
    "        if self.device.type == 'cpu':\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=1e-5)  # –ú–µ–Ω—å—à–∏–π lr –¥–ª—è CPU\n",
    "        else:\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=2e-5)\n",
    "\n",
    "    def fit(self):\n",
    "        self.model.train()\n",
    "        losses = 0\n",
    "        \n",
    "        for data in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            input_ids = data[\"input_ids\"].to(self.device)\n",
    "            attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "            labels = data[\"labels\"].to(self.device)\n",
    "            \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            losses += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        train_loss = losses / len(self.train_loader)\n",
    "        return train_loss\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        threshold = 0.2\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in self.valid_loader:\n",
    "                input_ids = data[\"input_ids\"].to(self.device)\n",
    "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "                labels = data[\"labels\"].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= threshold).float().cpu().numpy()\n",
    "                \n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã F1-–º–µ—Ç—Ä–∏–∫\n",
    "        f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        print(f\"F1-micro: {f1_micro:.4f}, F1-macro: {f1_macro:.4f}\")\n",
    "        return f1_micro, f1_macro\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
    "            train_loss = self.fit()\n",
    "            print(f'Train loss {train_loss}')\n",
    "            if hasattr(self, 'valid_loader'):\n",
    "                f1_micro, f1_macro = self.evaluate()\n",
    "                print(f'Validation F1 micro: {f1_micro}, F1 macro: {f1_macro}')\n",
    "\n",
    "        self.model.save_pretrained(self.model_save_path)\n",
    "        self.tokenizer.save_pretrained(self.tokenizer_save_path)\n",
    "\n",
    "\n",
    "    def predict(self, texts: list, threshold=0.5):\n",
    "\n",
    "        self.model.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏\n",
    "        batch_size = 16\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                encoding = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    max_length=self.max_len,\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    return_tensors='pt',\n",
    "                ).to(self.device)\n",
    "                \n",
    "                outputs = self.model(**encoding)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # –ü—Ä–∏–º–µ–Ω—è–µ–º sigmoid –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "                probabilities = torch.sigmoid(logits)\n",
    "                \n",
    "                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "                batch_preds = (probabilities >= threshold).cpu().numpy()\n",
    "                all_predictions.extend(batch_preds)\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "        result = []\n",
    "        for pred in all_predictions:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤, –≥–¥–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ—Ä–æ–≥\n",
    "            class_indices = np.where(pred)[0].tolist()\n",
    "            result.append(class_indices)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–µ–ª–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n",
    "- –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "- —Ä–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä—ã\n",
    "- –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –≤ –±–∏–Ω–∞—Ä–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É —Å –ø–æ–º–æ—â—å—é MultiLabelBinarizer –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–≤–æ–∞–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: (6000, 2)\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: 6\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
      "class\n",
      "1    1000\n",
      "2    1000\n",
      "3    1000\n",
      "4    1000\n",
      "5    1000\n",
      "6    1000\n",
      "Name: count, dtype: int64\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏: 5620\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: 6\n",
      "–§–æ—Ä–º–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: (4496, 6)\n"
     ]
    }
   ],
   "source": [
    "df = result_df\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(f\"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: {df['class'].nunique()}\")\n",
    "print(f\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\\n{df['class'].value_counts()}\")\n",
    "\n",
    "\n",
    "grouped_texts = df.groupby('text')['class'].apply(list).reset_index()\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏: {len(grouped_texts)}\")\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    grouped_texts['text'], \n",
    "    grouped_texts['class'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_binary = mlb.fit_transform(y_train)\n",
    "y_test_binary = mlb.transform(y_test)\n",
    "\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {len(mlb.classes_)}\")\n",
    "print(f\"–§–æ—Ä–º–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {y_train_binary.shape}\")\n",
    "with open(os.path.join('..', 'models', 'label_binarizer.pkl'), 'wb') as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–≥–ª–∞—Å–Ω–æ –æ—Ç–ª–∞–¥–æ—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º, —Ç–µ–∫—Å—Ç—ã —Ä–∞–∑–¥–µ–ª–∏–ª–∏—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º —Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥, —É–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏, –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –∏ –¥–µ–ª–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º –∏–∑ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ CPU: 8\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avmar\\AppData\\Local\\Temp\\ipykernel_34928\\3590057294.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.36228467091864963\n",
      "F1-micro: 0.6510, F1-macro: 0.6590\n",
      "Validation F1 micro: 0.6510044262853252, F1 macro: 0.6590245423677097\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avmar\\AppData\\Local\\Temp\\ipykernel_34928\\3590057294.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.23961691036024976\n",
      "F1-micro: 0.7051, F1-macro: 0.7105\n",
      "Validation F1 micro: 0.7051094890510949, F1 macro: 0.7105033759792637\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avmar\\AppData\\Local\\Temp\\ipykernel_34928\\3590057294.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.17839611819497844\n",
      "F1-micro: 0.7173, F1-macro: 0.7199\n",
      "Validation F1 micro: 0.7172573189522342, F1 macro: 0.7199361949002757\n",
      "–¢–µ–∫—Å—Ç: –†–µ—à–∏–ª–∏ –ø—Ä–æ–≥—É–ª—è—Ç—å—Å—è —Å —Å–µ–º—å–µ–π –ø–æ —Ç–æ—Ä–≥–æ–≤–æ–º—É —Ü–µ–Ω—Ç—Ä—É. –û–±–æ–∂–∞—é —Ç–∞–∫–∏–µ —Å–µ–º–µ–π–Ω—ã–µ –ø–æ—Å–∏–¥–µ–ª–∫–∏ –∏ –≤—Å–µ —Ç–∞–∫–æ–µ. –ö–æ–≥–¥–∞ –ø—Ä–æ–≥–æ–ª–æ–¥–∞–ª–∏—Å—å, –≤—Å–ø–æ–º–Ω–∏–ª–∏ –ø—Ä–æ –í–∫—É—Å—Å-–í–∏–ª–ª–∞. –í –Ω–µ–º –≤—Å–µ–≥–¥–∞ –≤—Å–µ —Å–≤–µ–∂–µ–µ –∏ –ø–æ–ª–µ–∑–Ω–æ–µ\n",
      "–¢–µ–∫—Å—Ç –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –Ω–∏ –∫ –æ–¥–Ω–æ–º—É –∏–∑ –∫–ª–∞—Å—Å–æ–≤\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "classifier = BertClassifier(\n",
    "    model_path=\"DeepPavlov/rubert-base-cased\",\n",
    "    tokenizer_path=\"DeepPavlov/rubert-base-cased\",\n",
    "    n_classes=len(mlb.classes_),\n",
    "    epochs=3,\n",
    "    model_save_path=os.path.join('..', 'models', 'rubert_hackothon'),\n",
    "    tokenizer_save_path=os.path.join('..', 'models', 'rubert_hackothon_tokenizer')\n",
    ")\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "classifier.preparation(\n",
    "    X_train.tolist(),\n",
    "    y_train_binary.tolist(), \n",
    "    X_test.tolist(),\n",
    "    y_test_binary.tolist()  \n",
    ")\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "classifier.train()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "texts_to_predict = [\"–†–µ—à–∏–ª–∏ –ø—Ä–æ–≥—É–ª—è—Ç—å—Å—è —Å —Å–µ–º—å–µ–π –ø–æ —Ç–æ—Ä–≥–æ–≤–æ–º—É —Ü–µ–Ω—Ç—Ä—É. –û–±–æ–∂–∞—é —Ç–∞–∫–∏–µ —Å–µ–º–µ–π–Ω—ã–µ –ø–æ—Å–∏–¥–µ–ª–∫–∏ –∏ –≤—Å–µ —Ç–∞–∫–æ–µ. –ö–æ–≥–¥–∞ –ø—Ä–æ–≥–æ–ª–æ–¥–∞–ª–∏—Å—å, –≤—Å–ø–æ–º–Ω–∏–ª–∏ –ø—Ä–æ –í–∫—É—Å—Å-–í–∏–ª–ª–∞. –í –Ω–µ–º –≤—Å–µ–≥–¥–∞ –≤—Å–µ —Å–≤–µ–∂–µ–µ –∏ –ø–æ–ª–µ–∑–Ω–æ–µ\"]\n",
    "predictions = classifier.predict(texts_to_predict)\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤\n",
    "for text, pred_indices in zip(texts_to_predict, predictions):\n",
    "    if pred_indices:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–ª–∞—Å—Å\n",
    "        pred_classes = [mlb.classes_[idx] for idx in pred_indices]\n",
    "        print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã: {pred_classes}\")\n",
    "    else:\n",
    "        print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "        print(\"–¢–µ–∫—Å—Ç –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –Ω–∏ –∫ –æ–¥–Ω–æ–º—É –∏–∑ –∫–ª–∞—Å—Å–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å—Ö–æ–¥—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –≤—ã–≤–æ–¥—ã:\n",
    "- \"–ü–æ—Ç–µ—Ä–∏\" —É–º–µ–Ω—å—à–∞–ª–∏—Å—å –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —ç–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è (—Å 0.36 –¥–æ 0.18)\n",
    "- –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º F1 micro –∏ F1 macro —É–≤–µ–ª–∏—á–∏–≤–∞–ª–∏—Å—å –Ω–∞ –≤—Å–µ–º –ø—É—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–∏–µ–º–ª–∏–º—É—é —Ç–æ—á–Ω–æ—Å—Ç—å\n",
    "- –ú–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç—Ä–∞–±–æ—Ç–∞–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –Ω–µ –æ–ø—Ä–µ–¥–∏–ª –∫–∞—Ç–µ–≥–æ—Ä–∏—é. –î–∞–Ω–Ω—É—é —á–∞—Å—Ç—å —Ä–µ—à–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–∏.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

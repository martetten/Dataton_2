{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–º–ø–æ—Ä—Ç –±–∏–±–∏–ª–∏–æ—Ç–µ–∫   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/raw/1.csv')\n",
    "df2 = pd.read_csv('../data/raw/2.csv')\n",
    "df3 = pd.read_csv('../data/raw/3.csv')\n",
    "df4 = pd.read_csv('../data/raw/4.csv')\n",
    "df5 = pd.read_csv('../data/raw/5.csv')\n",
    "df6 = pd.read_csv('../data/raw/6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   568 non-null    object\n",
      " 2   speech2text  338 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   418 non-null    object\n",
      " 2   speech2text  191 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   521 non-null    object\n",
      " 2   speech2text  202 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   678 non-null    object\n",
      " 2   speech2text  342 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   622 non-null    object\n",
      " 2   speech2text  202 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   doc_text     1000 non-null   object\n",
      " 1   image2text   613 non-null    object\n",
      " 2   speech2text  596 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df1.info())\n",
    "print(df2.info())\n",
    "print(df3.info())\n",
    "print(df4.info())\n",
    "print(df5.info())\n",
    "print(df6.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_text</th>\n",
       "      <th>image2text</th>\n",
       "      <th>speech2text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚≠êÔ∏è  –ö–Ω–æ–ø–∫–∞: ‚≠êÔ∏èSTART‚≠êÔ∏è(https://t.me/major/start...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ê –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ü—Ä–∞–≤–∏–ª—å–Ω–æ. –í –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...</td>\n",
       "      <td>–¥–µ–≤—á–æ–Ω–∫–∏ –Ω–µ —É–º–µ—é—Ç –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å—Å—è sanille –æ–Ω–∏ —ç...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–¢–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...</td>\n",
       "      <td>10:42 nuil –ø—É–ª telegram ^ 51 142 –ø–æ–¥–ø–∏—Å—á–∏–∫–∞ 12...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–£ –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...</td>\n",
       "      <td>—Ç —Å –Ω–µ ^ –µ z 8 * \\\"8 –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∞ –æ —Å—Ç–µ–Ω–¥–∞–ø...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>–û—Ç–∫–∞–∑–∞–ª–∏ –Ω–æ–≥–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è!  #—â–µ—Ä–±–∞–∫–æ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–ù–æ—á—å, –æ–∂–∏–¥–∞–Ω–∏—è, —Ö–æ–ª–æ–¥, –±–æ–ª—å, —Å–ª–æ–≤–Ω–æ —è —Ä–∞—Å–∫–æ–ª, ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>–û—Å—Ç–æ—Ä–æ–∂–Ω–æ –ø—Ä–æ–ø–∞–ª–∞ —Å–æ–±–∞–∫–∞! #–∞–ª–µ–∫—Å–µ–π—â–µ—Ä–±–∞–∫–æ–≤ #—â–µ...</td>\n",
       "      <td>(–ø–∞–ø—É—Å–æ–≤^ —Ö</td>\n",
       "      <td>–í—Å–µ–≥–¥–∞ –æ—á–µ–Ω—å –æ–±–∏–¥–Ω–æ —á–∏—Ç–∞—Ç—å –ø–æ–¥–æ–±–Ω—ã–µ —Ä–æ–¥–Ω—ã–µ –æ–±—ä...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>–î–ª—è –≤–∞—à–∏—Ö –æ–≥–æ–Ω—ë—á–∫–æ–≤üî•</td>\n",
       "      <td>v –∫ –≤–∏–¥–µ–æ</td>\n",
       "      <td>–Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞—Ö –æ–≥–æ–Ω—ë—á–∫–∏ –≤–¥—Ä—É–≥ –∑–∞–∂–∏–≥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∑–∞...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>#–∑–∏–º–∏–Ω–∞—Ä—É–ª–∏—Ç üòÇüòÇüòÇ #–∞–∫—Ç—Ä–∏—Å–∞—Ç–µ–∞—Ç—Ä–∞–∏–∫–∏–Ω–æ  #–≤–∏–∫—Ç–æ—Ä–∏...</td>\n",
       "      <td>–æ 6</td>\n",
       "      <td>–ß—Ç–æ –≤—ã —É–º–µ–µ—Ç–µ –¥–µ–ª–∞—Ç—å? –í—Å—ë. –°—Ç—Ä–µ–ª—è—Ç—å, –≤–∞—Ä–∏—Ç—å —Ö–∞...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>–î–µ–≤–æ—á–∫–∏, –Ω—É –º—ã?!  @katiashmatia –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—Å—Ç...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–º–∏–ª–∞—è –æ–Ω –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –º–Ω–µ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ —á–µ—Ç—ã...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               doc_text  \\\n",
       "0     –¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...   \n",
       "1     ‚≠êÔ∏è  –ö–Ω–æ–ø–∫–∞: ‚≠êÔ∏èSTART‚≠êÔ∏è(https://t.me/major/start...   \n",
       "2     –ê –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ü—Ä–∞–≤–∏–ª—å–Ω–æ. –í –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...   \n",
       "3     –¢–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...   \n",
       "4     –£ –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...   \n",
       "...                                                 ...   \n",
       "5995  –û—Ç–∫–∞–∑–∞–ª–∏ –Ω–æ–≥–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è!  #—â–µ—Ä–±–∞–∫–æ...   \n",
       "5996  –û—Å—Ç–æ—Ä–æ–∂–Ω–æ –ø—Ä–æ–ø–∞–ª–∞ —Å–æ–±–∞–∫–∞! #–∞–ª–µ–∫—Å–µ–π—â–µ—Ä–±–∞–∫–æ–≤ #—â–µ...   \n",
       "5997                               –î–ª—è –≤–∞—à–∏—Ö –æ–≥–æ–Ω—ë—á–∫–æ–≤üî•   \n",
       "5998  #–∑–∏–º–∏–Ω–∞—Ä—É–ª–∏—Ç üòÇüòÇüòÇ #–∞–∫—Ç—Ä–∏—Å–∞—Ç–µ–∞—Ç—Ä–∞–∏–∫–∏–Ω–æ  #–≤–∏–∫—Ç–æ—Ä–∏...   \n",
       "5999  –î–µ–≤–æ—á–∫–∏, –Ω—É –º—ã?!  @katiashmatia –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—Å—Ç...   \n",
       "\n",
       "                                             image2text  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2     –¥–µ–≤—á–æ–Ω–∫–∏ –Ω–µ —É–º–µ—é—Ç –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å—Å—è sanille –æ–Ω–∏ —ç...   \n",
       "3     10:42 nuil –ø—É–ª telegram ^ 51 142 –ø–æ–¥–ø–∏—Å—á–∏–∫–∞ 12...   \n",
       "4     —Ç —Å –Ω–µ ^ –µ z 8 * \\\"8 –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–ª–∞ –æ —Å—Ç–µ–Ω–¥–∞–ø...   \n",
       "...                                                 ...   \n",
       "5995                                                NaN   \n",
       "5996                                        (–ø–∞–ø—É—Å–æ–≤^ —Ö   \n",
       "5997                                          v –∫ –≤–∏–¥–µ–æ   \n",
       "5998                                                –æ 6   \n",
       "5999                                                NaN   \n",
       "\n",
       "                                            speech2text  class  \n",
       "0                                                   NaN      1  \n",
       "1                                                   NaN      1  \n",
       "2                                                   NaN      1  \n",
       "3                                                   NaN      1  \n",
       "4                                                   NaN      1  \n",
       "...                                                 ...    ...  \n",
       "5995  –ù–æ—á—å, –æ–∂–∏–¥–∞–Ω–∏—è, —Ö–æ–ª–æ–¥, –±–æ–ª—å, —Å–ª–æ–≤–Ω–æ —è —Ä–∞—Å–∫–æ–ª, ...      6  \n",
       "5996  –í—Å–µ–≥–¥–∞ –æ—á–µ–Ω—å –æ–±–∏–¥–Ω–æ —á–∏—Ç–∞—Ç—å –ø–æ–¥–æ–±–Ω—ã–µ —Ä–æ–¥–Ω—ã–µ –æ–±—ä...      6  \n",
       "5997  –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞—Ö –æ–≥–æ–Ω—ë—á–∫–∏ –≤–¥—Ä—É–≥ –∑–∞–∂–∏–≥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∑–∞...      6  \n",
       "5998  –ß—Ç–æ –≤—ã —É–º–µ–µ—Ç–µ –¥–µ–ª–∞—Ç—å? –í—Å—ë. –°—Ç—Ä–µ–ª—è—Ç—å, –≤–∞—Ä–∏—Ç—å —Ö–∞...      6  \n",
       "5999  –º–∏–ª–∞—è –æ–Ω –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –º–Ω–µ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ —á–µ—Ç—ã...      6  \n",
       "\n",
       "[6000 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['class'] = 1\n",
    "df2['class'] = 2\n",
    "df3['class'] = 3\n",
    "df4['class'] = 4\n",
    "df5['class'] = 5\n",
    "df6['class'] = 6\n",
    "df_gen = pd.concat([df1, df2, df3, df4, df5, df6]).reset_index(drop=True)\n",
    "df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  class\n",
      "0     –¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...      1\n",
      "1     ‚≠êÔ∏è  –ö–Ω–æ–ø–∫–∞: ‚≠êÔ∏èSTART‚≠êÔ∏è(https://t.me/major/start...      1\n",
      "2     –ê –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ü—Ä–∞–≤–∏–ª—å–Ω–æ. –í –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...      1\n",
      "3     –¢–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...      1\n",
      "4     –£ –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...      1\n",
      "...                                                 ...    ...\n",
      "5995  –û—Ç–∫–∞–∑–∞–ª–∏ –Ω–æ–≥–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è!  #—â–µ—Ä–±–∞–∫–æ...      6\n",
      "5996  –û—Å—Ç–æ—Ä–æ–∂–Ω–æ –ø—Ä–æ–ø–∞–ª–∞ —Å–æ–±–∞–∫–∞! #–∞–ª–µ–∫—Å–µ–π—â–µ—Ä–±–∞–∫–æ–≤ #—â–µ...      6\n",
      "5997  –î–ª—è –≤–∞—à–∏—Ö –æ–≥–æ–Ω—ë—á–∫–æ–≤üî• | v –∫ –≤–∏–¥–µ–æ | –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞...      6\n",
      "5998  #–∑–∏–º–∏–Ω–∞—Ä—É–ª–∏—Ç üòÇüòÇüòÇ #–∞–∫—Ç—Ä–∏—Å–∞—Ç–µ–∞—Ç—Ä–∞–∏–∫–∏–Ω–æ  #–≤–∏–∫—Ç–æ—Ä–∏...      6\n",
      "5999  –î–µ–≤–æ—á–∫–∏, –Ω—É –º—ã?!  @katiashmatia –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ—Å—Ç...      6\n",
      "\n",
      "[6000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df_gen.replace('NaN', np.nan)\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ —Ç—Ä–∏ –∫–æ–ª–æ–Ω–∫–∏\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ —Å —É—á–µ—Ç–æ–º NaN –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "def combine_columns(row):\n",
    "    values = [row['doc_text'], row['image2text'], row['speech2text']]\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏ NaN –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    values = [str(v) for v in values if v is not None and not (isinstance(v, float) and np.isnan(v))]\n",
    "    return ' | '.join(values) if values else np.nan\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º\n",
    "df['text'] = df.apply(combine_columns, axis=1)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π DataFrame —Ç–æ–ª—å–∫–æ —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π –∏ –∫–ª–∞—Å—Å–æ–º\n",
    "result_df = df[['text', 'class']]\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(result_df)\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ñ–∞–π–ª\n",
    "# result_df.to_csv('–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π_–¥–∞—Ç–∞—Å–µ—Ç.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vladimir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vladimir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import emoji\n",
    "import inflect\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "def clean_text(text):\n",
    "    text = base_clean_text(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def base_clean_text(text: str):\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    temp = inflect.engine()\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = re.sub('http\\S+', '', word)\n",
    "        word = re.sub('[^\\s–∞-—è—ë–Å–ê-–Øa-zA-Z]', '', word)\n",
    "        if word.isdigit():\n",
    "            words.append(temp.number_to_words(word))\n",
    "        else:\n",
    "            if word not in stop_words:\n",
    "                words.append(word)\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "def emojis_words(text):\n",
    "    # –ú–æ–¥—É–ª—å emoji: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∂–∏ –≤ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç—ë–º –∑–∞–º–µ–Ω—ã \":\" –∏\" _\", –∞ —Ç–∞–∫ –∂–µ - –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–µ–ª–∞ –º–µ–∂–¥—É –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏\n",
    "    text = text.replace(\":\", \"\").replace(\"_\", \" \")\n",
    "    return text\n",
    "def stem_russian_text(text):\n",
    "    stemmer = RussianStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        if re.match('[–∞-—è–ê-–Ø]', word):\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            stemmed_words.append(stemmed_word)\n",
    "        else:\n",
    "            stemmed_words.append(word)\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.FloatTensor(self.labels[idx])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º FloatTensor –¥–ª—è –º—É–ª—å—Ç–∏–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class BertClassifier:\n",
    "\n",
    "    def __init__(self, model_path, tokenizer_path, n_classes=44, epochs=7, \n",
    "                 model_save_path='model_save', tokenizer_save_path='tokenizer_path', \n",
    "                 force_cpu=False, batch_size=16, max_len=512):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model_save_path = model_save_path\n",
    "        self.tokenizer_save_path = tokenizer_save_path\n",
    "        self.max_len = max_len\n",
    "        self.epochs = epochs\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "        if force_cpu:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\")\n",
    "        else:\n",
    "            # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ GPU, –Ω–æ –±—É–¥–µ–º –≥–æ—Ç–æ–≤—ã –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ CPU\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                    # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ GPU\n",
    "                    self.model = BertForSequenceClassification.from_pretrained(\n",
    "                        model_path, \n",
    "                        num_labels=n_classes,\n",
    "                        problem_type=\"multi_label_classification\"\n",
    "                    ).to(self.device)\n",
    "                    # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏, –≤—ã–¥–µ–ª–∏–≤ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä\n",
    "                    test_tensor = torch.zeros((batch_size, max_len), device=self.device)\n",
    "                    del test_tensor  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "                    print(\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU\")\n",
    "                else:\n",
    "                    self.device = torch.device(\"cpu\")\n",
    "                    print(\"GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU\")\n",
    "                    self.model = BertForSequenceClassification.from_pretrained(\n",
    "                        model_path, \n",
    "                        num_labels=n_classes,\n",
    "                        problem_type=\"multi_label_classification\"\n",
    "                    ).to(self.device)\n",
    "            except (RuntimeError, torch.cuda.OutOfMemoryError):\n",
    "                # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU\n",
    "                print(\"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU, –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU\")\n",
    "                torch.cuda.empty_cache()  # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU\n",
    "                self.device = torch.device(\"cpu\")\n",
    "                # –ó–∞–Ω–æ–≤–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU\n",
    "                self.model = BertForSequenceClassification.from_pretrained(\n",
    "                    model_path, \n",
    "                    num_labels=n_classes,\n",
    "                    problem_type=\"multi_label_classification\"\n",
    "                ).to(self.device)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.model_path, num_labels=self.n_classes)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def tokenize_texts(self, texts, tokenizer, max_length):\n",
    "        tokenized_texts = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        return tokenized_texts\n",
    "\n",
    "    def preparation(self, X_train, y_train, X_valid, y_valid):\n",
    "        # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã, –∫–∞–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö\n",
    "        train_encodings = self.tokenize_texts(X_train, self.tokenizer, self.max_len)\n",
    "        valid_encodings = self.tokenize_texts(X_valid, self.tokenizer, self.max_len)\n",
    "        \n",
    "        # create datasets\n",
    "        self.train_set = CustomDataset(train_encodings, y_train)\n",
    "        self.valid_set = CustomDataset(valid_encodings, y_valid)\n",
    "\n",
    "        # create data loaders —Å –º–µ–Ω—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ –Ω–∞ CPU\n",
    "        if self.device.type == 'cpu':\n",
    "            # –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è CPU\n",
    "            cpu_batch_size = max(1, self.batch_size // 2)\n",
    "            print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ CPU: {cpu_batch_size}\")\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=cpu_batch_size)\n",
    "            self.valid_loader = DataLoader(self.valid_set, batch_size=cpu_batch_size)\n",
    "        else:\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size)\n",
    "            self.valid_loader = DataLoader(self.valid_set, batch_size=self.batch_size)\n",
    "\n",
    "        # helpers initialization —Å –º–µ–Ω—å—à–∏–º learning rate –Ω–∞ CPU\n",
    "        if self.device.type == 'cpu':\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=1e-5)  # –ú–µ–Ω—å—à–∏–π lr –¥–ª—è CPU\n",
    "        else:\n",
    "            self.optimizer = AdamW(self.model.parameters(), lr=2e-5)\n",
    "\n",
    "    def fit(self):\n",
    "        self.model.train()\n",
    "        losses = 0\n",
    "        \n",
    "        for data in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            input_ids = data[\"input_ids\"].to(self.device)\n",
    "            attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "            labels = data[\"labels\"].to(self.device)\n",
    "            \n",
    "            # –î–ª—è –º—É–ª—å—Ç–∏–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ labels –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å float –∏ –∏–º–µ—Ç—å —Ñ–æ—Ä–º—É [batch_size, n_classes]\n",
    "            # –ï—Å–ª–∏ –≤–∞—à–∏ –º–µ—Ç–∫–∏ –Ω–µ –≤ —Ç–∞–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –∏—Ö –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å\n",
    "            \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            losses += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        train_loss = losses / len(self.train_loader)\n",
    "        return train_loss\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        threshold = 0.2\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in self.valid_loader:\n",
    "                input_ids = data[\"input_ids\"].to(self.device)\n",
    "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "                labels = data[\"labels\"].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= threshold).float().cpu().numpy()\n",
    "                \n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã F1-–º–µ—Ç—Ä–∏–∫\n",
    "        f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        print(f\"F1-micro: {f1_micro:.4f}, F1-macro: {f1_macro:.4f}\")\n",
    "        return f1_micro, f1_macro\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
    "            train_loss = self.fit()\n",
    "            print(f'Train loss {train_loss}')\n",
    "            if hasattr(self, 'valid_loader'):\n",
    "                f1_micro, f1_macro = self.evaluate()\n",
    "                print(f'Validation F1 micro: {f1_micro}, F1 macro: {f1_macro}')\n",
    "\n",
    "        self.model.save_pretrained(self.model_save_path)\n",
    "        self.tokenizer.save_pretrained(self.tokenizer_save_path)\n",
    "\n",
    "\n",
    "    def predict(self, texts: list, threshold=0.5):\n",
    "        \"\"\"\n",
    "        –ú–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–µ—Ç–æ–∫ –≤ –∑–∞–¥–∞—á–µ –º—É–ª—å—Ç–∏–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "            threshold (float): –ü–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è –º–µ—Ç–∫–∏ (0.0-1.0)\n",
    "            \n",
    "        Returns:\n",
    "            list: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∏–Ω–¥–µ–∫—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "                –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –Ω–∏ –∫ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Å—É\n",
    "        \"\"\"\n",
    "        self.model.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏\n",
    "        batch_size = 16\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                encoding = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    max_length=self.max_len,\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    return_tensors='pt',\n",
    "                ).to(self.device)\n",
    "                \n",
    "                outputs = self.model(**encoding)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # –ü—Ä–∏–º–µ–Ω—è–µ–º sigmoid –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "                probabilities = torch.sigmoid(logits)\n",
    "                \n",
    "                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "                batch_preds = (probabilities >= threshold).cpu().numpy()\n",
    "                all_predictions.extend(batch_preds)\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "        result = []\n",
    "        for pred in all_predictions:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤, –≥–¥–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ—Ä–æ–≥\n",
    "            class_indices = np.where(pred)[0].tolist()\n",
    "            result.append(class_indices)  # –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º, –µ—Å–ª–∏ –Ω–µ—Ç –∫–ª–∞—Å—Å–æ–≤\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: (6000, 2)\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: 6\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
      "class\n",
      "1    1000\n",
      "2    1000\n",
      "3    1000\n",
      "4    1000\n",
      "5    1000\n",
      "6    1000\n",
      "Name: count, dtype: int64\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏: 5620\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: 6\n",
      "–§–æ—Ä–º–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: (4496, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "df = result_df\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(f\"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: {df['class'].nunique()}\")\n",
    "print(f\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\\n{df['class'].value_counts()}\")\n",
    "\n",
    "\n",
    "grouped_texts = df.groupby('text')['class'].apply(list).reset_index()\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏: {len(grouped_texts)}\")\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    grouped_texts['text'], \n",
    "    grouped_texts['class'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_binary = mlb.fit_transform(y_train)\n",
    "y_test_binary = mlb.transform(y_test)\n",
    "\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {len(mlb.classes_)}\")\n",
    "print(f\"–§–æ—Ä–º–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {y_train_binary.shape}\")\n",
    "with open('label_binarizer.pkl', 'wb') as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m classifier = \u001b[43mBertClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDeepPavlov/rubert-base-cased\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# –∏–ª–∏ –¥—Ä—É–≥–∞—è –ø–æ–¥—Ö–æ–¥—è—â–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDeepPavlov/rubert-base-cased\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmlb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrubert_hackothon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_save_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrubert_hackothon_tokenizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\u001b[39;00m\n\u001b[32m     12\u001b[39m classifier.preparation(\n\u001b[32m     13\u001b[39m     X_train.tolist(),\n\u001b[32m     14\u001b[39m     y_train_binary.tolist(), \n\u001b[32m     15\u001b[39m     X_test.tolist(),\n\u001b[32m     16\u001b[39m     y_test_binary.tolist()  \n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mBertClassifier.__init__\u001b[39m\u001b[34m(self, model_path, tokenizer_path, n_classes, epochs, model_save_path, tokenizer_save_path, force_cpu, batch_size, max_len)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ GPU\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mBertForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmulti_label_classification\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏, –≤—ã–¥–µ–ª–∏–≤ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä\u001b[39;00m\n\u001b[32m     54\u001b[39m test_tensor = torch.zeros((batch_size, max_len), device=\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4638\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4635\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   4636\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4637\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m4638\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   4639\u001b[39m     )\n\u001b[32m   4641\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   4642\u001b[39m prefix = model.base_model_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:556\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    550\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(checkpoint_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    551\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m map_location != \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    552\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m version.parse(torch.__version__) >= version.parse(\u001b[33m\"\u001b[39m\u001b[33m2.1.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    553\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m is_zipfile(checkpoint_file)\n\u001b[32m    554\u001b[39m     ):\n\u001b[32m    555\u001b[39m         extra_args = {\u001b[33m\"\u001b[39m\u001b[33mmmap\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/torch/serialization.py:1516\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1521\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1523\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1524\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/torch/serialization.py:2114\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2112\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2113\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2114\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2115\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2117\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/torch/_weights_only_unpickler.py:532\u001b[39m, in \u001b[36mUnpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    525\u001b[39m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) > \u001b[32m0\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.serialization._maybe_decode_ascii(pid[\u001b[32m0\u001b[39m]) != \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ):\n\u001b[32m    529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[32m    530\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    531\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[32m0\u001b[39m], LONG_BINGET[\u001b[32m0\u001b[39m]]:\n\u001b[32m    534\u001b[39m     idx = (read(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[32m0\u001b[39m] == BINGET[\u001b[32m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, read(\u001b[32m4\u001b[39m)))[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/torch/serialization.py:2078\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   2076\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2077\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2078\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/Dataton_2/.venv/lib/python3.11/site-packages/torch/serialization.py:2031\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   2024\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_offset != zip_file.get_record_offset(name):\n\u001b[32m   2025\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2026\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mThis is a debug assert that was run as the `TORCH_SERIALIZATION_DEBUG` environment \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2027\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvariable was set: Incorrect offset for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_offset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m expected \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2028\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_file.get_record_offset(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2029\u001b[39m             )\n\u001b[32m   2030\u001b[39m     storage = (\n\u001b[32m-> \u001b[39m\u001b[32m2031\u001b[39m         \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2032\u001b[39m         ._typed_storage()\n\u001b[32m   2033\u001b[39m         ._untyped_storage\n\u001b[32m   2034\u001b[39m     )\n\u001b[32m   2035\u001b[39m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[32m   2036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "classifier = BertClassifier(\n",
    "    model_path=\"DeepPavlov/rubert-base-cased\", # –∏–ª–∏ –¥—Ä—É–≥–∞—è –ø–æ–¥—Ö–æ–¥—è—â–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "    tokenizer_path=\"DeepPavlov/rubert-base-cased\",\n",
    "    n_classes=len(mlb.classes_),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤\n",
    "    epochs=3,\n",
    "    model_save_path='rubert_hackothon',\n",
    "    tokenizer_save_path='rubert_hackothon_tokenizer'\n",
    ")\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "classifier.preparation(\n",
    "    X_train.tolist(),\n",
    "    y_train_binary.tolist(), \n",
    "    X_test.tolist(),\n",
    "    y_test_binary.tolist()  \n",
    ")\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "classifier.train()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "texts_to_predict = [\"–†–µ—à–∏–ª–∏ –ø—Ä–æ–≥—É–ª—è—Ç—å—Å—è —Å —Å–µ–º—å–µ–π –ø–æ —Ç–æ—Ä–≥–æ–≤–æ–º—É —Ü–µ–Ω—Ç—Ä—É. –û–±–æ–∂–∞—é —Ç–∞–∫–∏–µ —Å–µ–º–µ–π–Ω—ã–µ –ø–æ—Å–∏–¥–µ–ª–∫–∏ –∏ –≤—Å–µ —Ç–∞–∫–æ–µ. –ö–æ–≥–¥–∞ –ø—Ä–æ–≥–æ–ª–æ–¥–∞–ª–∏—Å—å, –≤—Å–ø–æ–º–Ω–∏–ª–∏ –ø—Ä–æ –í–∫—É—Å—Å-–í–∏–ª–ª–∞. –í –Ω–µ–º –≤—Å–µ–≥–¥–∞ –≤—Å–µ —Å–≤–µ–∂–µ–µ –∏ –ø–æ–ª–µ–∑–Ω–æ–µ\"]\n",
    "predictions = classifier.predict(texts_to_predict)\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤\n",
    "for text, pred_indices in zip(texts_to_predict, predictions):\n",
    "    if pred_indices:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–ª–∞—Å—Å\n",
    "        pred_classes = [mlb.classes_[idx] for idx in pred_indices]\n",
    "        print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã: {pred_classes}\")\n",
    "    else:\n",
    "        print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "        print(\"–¢–µ–∫—Å—Ç –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –Ω–∏ –∫ –æ–¥–Ω–æ–º—É –∏–∑ –∫–ª–∞—Å—Å–æ–≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "# # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ `dataset` - —ç—Ç–æ –≤–∞—à –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ \"–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è\" –∏ \"–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\"\n",
    "\n",
    "# # –°–æ–∑–¥–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "# label_mapping = {cat: i for i, cat in enumerate(result_df['class'].unique())}\n",
    "# dataset['label'] = dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].map(label_mapping)\n",
    "\n",
    "# # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     dataset['–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è'].tolist(), \n",
    "#     dataset['label'].tolist(),\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     stratify=dataset['label']  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫ –≤ –≤—ã–±–æ—Ä–∫–∞—Ö\n",
    "# )\n",
    "\n",
    "# # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "# # –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å BERT\n",
    "# model_path = \"DeepPavlov/rubert-base-cased\"  # –∏–ª–∏ –¥—Ä—É–≥—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å\n",
    "# tokenizer_path = \"DeepPavlov/rubert-base-cased\"\n",
    "# n_classes = len(label_mapping)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "\n",
    "# classifier = BertClassifier(\n",
    "#     model_path=model_path,\n",
    "#     tokenizer_path=tokenizer_path,\n",
    "#     n_classes=n_classes,\n",
    "#     epochs=5  # –ú–æ–∂–µ—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
    "# )\n",
    "\n",
    "# # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "# classifier.preparation(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# # 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "# classifier.train()\n",
    "\n",
    "# # 5. –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "# # –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –ø–µ—Ä–≤—ã—Ö 5 —Å–æ–æ–±—â–µ–Ω–∏–π:\n",
    "# sample_texts = dataset['–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è'].head(5).tolist()\n",
    "# predicted_categories = classifier.predict(sample_texts)\n",
    "\n",
    "# # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç–æ–∫ –æ–±—Ä–∞—Ç–Ω–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "# reverse_mapping = {i: cat for cat, i in label_mapping.items()}\n",
    "# predicted_category_names = [reverse_mapping.get(pred-1) for pred in predicted_categories]\n",
    "\n",
    "# print(\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:\", predicted_category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞\n",
    "# model_path = 'tiny_bert_44cat'\n",
    "# tokenizer_path = 'tiny_bert_44cat_tokenizer'\n",
    "\n",
    "# # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\n",
    "# classifier = BertClassifier(\n",
    "#     model_path=model_path,\n",
    "#     tokenizer_path=tokenizer_path,\n",
    "#     n_classes=6,\n",
    "# )\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n",
    "# classifier.load_model()\n",
    "\n",
    "# # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "# # –ï—Å–ª–∏ —É –≤–∞—Å –µ—â–µ –Ω–µ—Ç —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏, –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –µ–µ –∏–∑ –∏–º–µ—é—â–µ–≥–æ—Å—è –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # –°–æ–∑–¥–∞–¥–∏–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "# categories = dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].unique()\n",
    "# label_mapping = {cat: i for i, cat in enumerate(categories)}\n",
    "# reverse_mapping = {i: cat for i, cat in label_mapping.items()}\n",
    "\n",
    "# # –†–∞–∑–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ (–µ—Å–ª–∏ –µ—â–µ –Ω–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã)\n",
    "# _, X_test, _, y_test = train_test_split(\n",
    "#     dataset['–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è'].tolist(),\n",
    "#     dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].map(label_mapping).tolist(),\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     stratify=dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].map(label_mapping)\n",
    "# )\n",
    "\n",
    "# # 3. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "# # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "# all_predictions = []\n",
    "# for text in X_test:\n",
    "#     # predict –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "#     pred_indices = classifier.predict([text])\n",
    "#     # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é (—Å–∞–º—É—é –≤–µ—Ä–æ—è—Ç–Ω—É—é) –∫–∞—Ç–µ–≥–æ—Ä–∏—é\n",
    "#     all_predictions.append(pred_indices[0] - 1)  # –í—ã—á–∏—Ç–∞–µ–º 1, —Ç.–∫. predict –¥–æ–±–∞–≤–ª—è–µ—Ç 1\n",
    "\n",
    "# # 4. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n",
    "# accuracy = accuracy_score(y_test, all_predictions)\n",
    "# report = classification_report(y_test, all_predictions, target_names=categories)\n",
    "# conf_matrix = confusion_matrix(y_test, all_predictions)\n",
    "\n",
    "# print(f\"–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {accuracy:.4f}\")\n",
    "# print(\"\\n–û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\")\n",
    "# print(report)\n",
    "\n",
    "# print(\"\\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "#             xticklabels=categories, yticklabels=categories)\n",
    "# plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
    "# plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
    "# plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('confusion_matrix.png')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_categories = dataset['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'].unique()\n",
    "# label_mapping = {cat: i for i, cat in enumerate(original_categories)}\n",
    "# reverse_mapping = {i: cat for i, cat in label_mapping.items()}\n",
    "\n",
    "# print(\"–ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\")\n",
    "# for i, cat in sorted(reverse_mapping.items()):\n",
    "#     print(f\"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {i}: {cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = BertClassifier(\n",
    "#     model_path=model_path,\n",
    "#     tokenizer_path=tokenizer_path,\n",
    "#     n_classes=6,\n",
    "# )\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n",
    "# classifier.load_model()\n",
    "# classifier.predict('–°–µ–≥–æ–¥–Ω—è –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∞ –≤—Å—Ç—Ä–µ—á–∞ –ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—è –≥–æ—Å–¥—É–º—ã –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π —Ä–µ–≥–∏–æ–Ω–æ–≤')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
